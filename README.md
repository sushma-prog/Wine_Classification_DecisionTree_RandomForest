# ğŸ· Wine Classification using Decision Tree & Random Forest

A hands-on machine learning project to classify wine types using **Decision Tree** and **Random Forest** algorithms on the **UCI Wine Dataset**.

---

## ğŸ§  Project Overview

This project demonstrates how to build and evaluate two powerful classification models â€” **Decision Tree** and **Random Forest** â€” to predict wine categories based on their chemical attributes.

You will learn:

- Difference between Decision Tree and Random Forest
- When to use ensemble models
- How to visualize feature importance and decision boundaries
- How to evaluate classification models with Accuracy, Confusion Matrix, and more

---

## ğŸ“¦ Dataset Used

- **Dataset Name**: Wine Dataset  
- **Source**: `sklearn.datasets.load_wine()`  
- **Target classes**:
  - 0 = Class_0
  - 1 = Class_1
  - 2 = Class_2  
- **Features**: 13 numerical attributes (alcohol, malic acid, color intensity, etc.)

---

## ğŸ§ª Tools & Libraries

- Python
- NumPy, Pandas
- Matplotlib, Seaborn
- Scikit-learn

---

## ğŸ” Steps Performed

1. Loaded and explored the dataset
2. Performed EDA: histograms, boxplots, and heatmap
3. Split data into training and testing sets
4. Built and trained:
   - Decision Tree Classifier
   - Random Forest Classifier with 100 trees
5. Evaluated both models using:
   - Accuracy
   - Confusion Matrix
   - Classification Report
6. Plotted:
   - Feature Importances
   - A decision tree from the Random Forest

---

## ğŸ§  Model Evaluation

| Model              | Accuracy | Notes                             |
|-------------------|----------|-----------------------------------|
| Decision Tree      | ~94.4%   | Simple, interpretable             |
| Random Forest      | 100%     | Powerful ensemble, no overfitting |

âœ… Also included:
- Feature Importance Plot  
- Visualized one sample tree from the Random Forest

---

## ğŸ“Š Key Visuals

- ğŸ“Œ Feature Distribution
- ğŸ“Œ Class-wise Boxplots
- ğŸ“Œ Correlation Heatmap
- ğŸ“Œ Decision Tree Diagram
- ğŸ“Œ Feature Importances

---

## âœ… Final Notes

- Random Forest outperformed Decision Tree by reducing overfitting and increasing accuracy.
- Most important features: `flavanoids`, `color_intensity`, `proline`, `alcohol`.
- Great project to understand ensemble learning and model interpretability.

---

## ğŸ’¼ Author

**Sushma Sandanshiv**  
BTech Data Science Student | Machine Learning Enthusiast  
[GitHub Profile](https://github.com/sushma-prog) | [LinkedIn](https://www.linkedin.com/in/sushma-sandanshiv-2740422b7)

![image](https://github.com/user-attachments/assets/3044ff27-04a5-4ab5-aa71-a8c07330ad87)
![image](https://github.com/user-attachments/assets/cdf5ff63-3577-452a-bfed-77f1dfd06b70)
![image](https://github.com/user-attachments/assets/f3b38a9b-7ceb-4c11-8f11-092c996a5173)
![image](https://github.com/user-attachments/assets/3a5d1d32-1fee-475e-846f-999537a659b2)

